{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.ipynb","provenance":[],"authorship_tag":"ABX9TyPNd2ywvOPjTErVWjj6fD4T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rp1oWqkegahL","executionInfo":{"status":"ok","timestamp":1631525683749,"user_tz":-480,"elapsed":20237,"user":{"displayName":"Kaixi YI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15913186688303610504"}},"outputId":"3d285a2c-d5f7-401b-c826-d4a6bc45af9a"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VSeoxlsgdlM","executionInfo":{"status":"ok","timestamp":1631525684891,"user_tz":-480,"elapsed":5,"user":{"displayName":"Kaixi YI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15913186688303610504"}},"outputId":"fcb6213b-54a5-4898-db6e-c4e184b827d9"},"source":["!ls \"/content/drive/My Drive/Deep Learning based on python/\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["1.ipynb  4.5.3_tarin_nn_epoch.ipynb  gradient.py  mnist.py\n","2.ipynb  functions.py\t\t     mnist.pkl\t  __pycache__\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"Iw1hIzhagjnO","executionInfo":{"status":"error","timestamp":1631525806729,"user_tz":-480,"elapsed":35338,"user":{"displayName":"Kaixi YI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15913186688303610504"}},"outputId":"4f82edfa-4d1e-46aa-d7e4-f240e73fbcc6"},"source":["import sys, os\n","sys.path.append('/content/drive/My Drive/Deep Learning based on python/')\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mnist import load_mnist\n","from functions import *\n","from gradient import numerical_gradient\n","\n","class TwolayerNet(object):\n","\t\"\"\"docstring for TwolayerNet\"\"\"\n","\t## 输入数据大小，隐藏层数据大小，输出层数据大小，\n","\tdef __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","\t\tself.params = {}\n","\t\tself.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\t# 高斯分布一个i*h大小的随机矩阵\n","\t\tself.params['b1'] = np.zeros(hidden_size)\t\t# 偏置都用0初始化\n","\t\tself.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","\t\tself.params['b2'] = np.zeros(output_size)\n","\n","\tdef predict(self, x):\n","\t\tW1, W2 = self.params['W1'], self.params['W2']\n","\t\tb1, b2 = self.params['b1'],\tself.params['b2']\n","\n","\t\ta1 = np.dot(x, W1) + b1\n","\t\tz1 = sigmoid(a1)\n","\t\ta2 = np.dot(z1, W2) + b2\n","\t\ty = softmax(a2)\n","\n","\t\treturn y\n","\n","\tdef loss(self, x, t):\n","\t\ty = self.predict(x)\n","\n","\t\treturn cross_entropy_error(y, t)\n","\n","\tdef accuracy(self, x, t):\n","\t\ty = self.predict(x)\n","\t\ty = np.argmax(y, axis=1)\n","\t\tt = np.argmax(t, axis=1)\n","\n","\t\taccuracy = np.sum(y == t) / float(x.shape[0])\n","\t\treturn accuracy\n","\n","\tdef numerical_gradient(self, x, t):\n","\t\tloss_W = lambda W: self.loss(x, t)\n","\n","\t\tgrads = {}\n","\t\tgrads['W1'] = numerical_gradient(loss_W, self.params['W1'])\t\t#这里numerical_gradient依然是用的数值微分来做梯度\n","\t\tgrads['b1'] = numerical_gradient(loss_W, self.params['b1'])\t\t#之后的章节会学习反向传播法来做更快的梯度计算\n","\t\tgrads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","\t\tgrads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","\t\treturn grads\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","#超参数\n","train_size = x_train.shape[0]\t# 60000\n","batch_size = 100 \t\t# mini batch的大小为100 ，这样相当于每次要从60000个训练数据中随机取出100个\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","# 平均每个epoch的重复次数\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","#超参数\n","iters_num = 10000\t\t# 梯度更新的次数，此处为10000次\n","learning_rate = 0.1\n","network = TwolayerNet(input_size=784, hidden_size=50,output_size=10)\n","\n","for i in range(iters_num):\n","\t#\t获取mini-batch\n","\tbatch_mask = np.random.choice(train_size, batch_size)\t# 从train_size这么多个数据（60000）中随机抽取batch_size这么多个数据(100)个\n","\tx_batch = x_train[batch_mask]\n","\tt_batch = t_train[batch_mask]\n","\n","\t#\t计算梯度\n","\tgrad = network.numerical_gradient(x_batch, t_batch)\n","\t# grad = network.gradient(x_batch,t_batch)\t# 高速版\n","\n","\t# 梯度下降，因为这里mini batch是随机选的，所以是SGD方法，随机梯度下降法\n","\tfor key in ('W1', 'b1', 'W2', 'b2'):\n","\t\tnetwork.params[key] -= learning_rate * grad[key]\n","\n","\t# 记录学习过程，记录每一次的loss\n","\tloss = network.loss(x_batch, t_batch)\n","\ttrain_loss_list.append(loss)\n","\n","#\tprint(\"i: \",i)\n","\n","\t#计算每个epoch的识别精度,每600次循环为一个epoch\n","\tif i % iter_per_epoch == 0:\n","\t\ttrain_acc = network.accuracy(x_train, t_train)\n","\t\ttest_acc = network.accuracy(x_test, t_test)\n","\t\ttrain_acc_list.append(train_acc)\n","\t\ttest_acc_list.append(test_acc)\n","\t\tprint(\"train acc, test acc | \" + str(train_acc) + \",\" + str(test_acc))\n","\n","# 绘制图形\n","markers = {'train': 'o', 'test': 's'}\n","x = np.arange(len(train_acc_list))\n","plt.plot(x, train_acc_list, label='train acc')\n","plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"accuracy\")\n","plt.ylim(0, 1.0)\n","plt.legend(loc='lower right')\n","plt.show()"],"execution_count":3,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-dd0c49a55c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m#       计算梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m# grad = network.gradient(x_batch,t_batch)      # 高速版\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-dd0c49a55c80>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m#这里numerical_gradient依然是用的数值微分来做梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m#之后的章节会学习反向传播法来做更快的梯度计算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Deep Learning based on python/gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-dd0c49a55c80>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-dd0c49a55c80>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-dd0c49a55c80>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}